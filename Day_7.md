# scalars and vectors
Scalars are used to represent quantities that are one-dimensional, such as single values of data points, model parameters, or results of calculations. 
A vector is a one-dimensional array or list of numbers, and it represents a collection of related features or values in multiple dimensions. Vectors are used extensively to represent data points, model parameters, and intermediate calculations in algorithms.

# L1 Norm:
The L1 norm of a vector is the sum of the absolute values of its components. 

Example of L1 Norm:

For x=[3,âˆ’4,2], the L1 norm is:

âˆ¥
ğ‘¥
âˆ¥
1
=
âˆ£
3
âˆ£
+
âˆ£
âˆ’
4
âˆ£
+
âˆ£
2
âˆ£
=
3
+
4
+
2
=
9
âˆ¥xâˆ¥ 
1
â€‹
 =âˆ£3âˆ£+âˆ£âˆ’4âˆ£+âˆ£2âˆ£=3+4+2=9

 # L2 Norm:
 
The L2 norm of a vector is the square root of the sum of the squares of its components 

Example of L2 Norm:
For
x=[3,âˆ’4,2], the L2 norm is:

âˆ¥
ğ‘¥
âˆ¥
2
=
3
2
+
(
âˆ’
4
)
2
+
2
2
=
9
+
16
+
4
=
29
â‰ˆ
5.39
âˆ¥xâˆ¥ 
2
â€‹
 = root(
3^ 
2
 +(âˆ’4)^ 
2
 +2^ 
2
 )
â€‹
 = 
9+16+4
â€‹
 = 
29
â€‹
 â‰ˆ5.39


# Eigenvalue:

An eigenvalue is a scalar 
Î» that tells you how much a matrix stretches or compresses vectors when the matrix is applied to them. More formally:

For a given square matrix 
ğ´, an eigenvalue 
ğœ† is a scalar that satisfies the equation:
ğ´
ğ‘£
=
ğœ†
ğ‘£
Av=Î»v
where:
ğ´ is a square matrix (e.g., 
ğ‘›
Ã—
ğ‘›
nÃ—n),
ğ‘£ is a vector (called an eigenvector),
Î» is the eigenvalue

# Eigenvector:

An eigenvector is a non-zero vector 
ğ‘£ that remains in the same direction (or opposite direction) after the matrix transformation. Essentially, applying the matrix 
ğ´ to the eigenvector 
ğ‘£ only scales it by a factor of 
Î», without changing its direction.

Mathematically, the eigenvector 
ğ‘£
v satisfies the equation:
ğ´
ğ‘£
=
ğœ†
ğ‘£
Av=Î»v
where 
v is a vector that is not the zero vector. 

# Types of matrix

Square Matrix: Same number of rows and columns.

Row Matrix: One row, multiple columns.

Column Matrix: One column, multiple rows.

Zero Matrix: All elements are zero.

Identity Matrix: Square matrix with ones on the diagonal and zeros elsewhere.

Diagonal Matrix: Square matrix with non-zero elements only on the diagonal.

Scalar Matrix: A diagonal matrix with identical diagonal elements.

Symmetric Matrix: A square matrix equal to its transpose.

Skew-Symmetric Matrix: A matrix whose transpose is the negative of itself.

# General Formula for Determinants (n x n Matrix)

For a general 
ğ‘›
Ã—
ğ‘›
nÃ—n matrix, the determinant is defined recursively using cofactor expansion. The determinant is calculated by expanding along any row or column, and for larger matrices, this involves calculating the determinants of smaller submatrices (minors).

Definition:
For an 
ğ‘›
Ã—
ğ‘›
nÃ—n matrix 
ğ´
A, the determinant is calculated as:

det
(ğ´)=
âˆ‘
(
âˆ’
1
)
^(ğ‘–
+
ğ‘—)
ğ‘
(ğ‘–
ğ‘—)
det
(
ğ´
ğ‘–
ğ‘—
)

# Steps to Find the Inverse of a 3x3 Matrix:

Calculate the determinant 
det
(
ğ´
).

Find the cofactor matrix of 
ğ´.


Transpose the cofactor matrix to get the adjugate (adjoint) matrix.

Multiply the adjugate by 
1
det
(
ğ´
).


â€‹
 

